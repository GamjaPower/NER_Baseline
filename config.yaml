CFG:
  # dataset arguments
  dset_name: "klue"
  task: "ner"
  num_labels: 13

  # training arguments
  DEBUG: false
  PLM: "klue/roberta-large"
  train_batch_size: 64
  valid_batch_size: 128
  num_epochs: 3 # validation loss is increasing after 5 epochs
  max_token_length: 256
  learning_rate: 0.00005 # has to be set as float explicitly due to https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number
  weight_decay: 0.001 # https://paperswithcode.com/method/weight-decay
  adam_beta_1: 0.9
  adam_beta_2: 0.98
  epsilon: 0.000000001
  fp16: false
  load_best_model_at_end: true
  gradient_accumulation_steps: 1
  num_checkpoints: 3
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  dropout: 0.1

  # inference arguments
  inference_model_name: "path"

  # wandb settings
  entity_name: "quoqa-nlp"
  project_name: "KLUE_NER"
